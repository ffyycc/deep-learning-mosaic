{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zY9P7VYyqRhH"
      },
      "source": [
        "\n",
        "## Import Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_YgmK1IolgcG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMcAh-nQqdJb"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LooppsJInm13",
        "outputId": "bd6a219c-3a75-4e8c-eab8-3e7a62b5c56b"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Filter for the classes we want (Cat, Dog, Airplane, Automobile)\n",
        "selected_classes = [3, 5, 0, 1]  # 3: cat, 5: dog, 0: airplane, 1: automobile in the original dataset\n",
        "\n",
        "# Create masks for train and test datasets\n",
        "train_mask = np.isin(train_labels, selected_classes).reshape(-1)\n",
        "test_mask = np.isin(test_labels, selected_classes).reshape(-1)\n",
        "\n",
        "# Apply masks to datasets\n",
        "train_images, train_labels = train_images[train_mask], train_labels[train_mask]\n",
        "test_images, test_labels = test_images[test_mask], test_labels[test_mask]\n",
        "\n",
        "# Map the labels to our new labels, and one-hot encoding labels\n",
        "label_dict = {3: 0, 5: 1, 0: 2, 1: 3}\n",
        "train_labels = tf.keras.utails.to_categorical([label_dict[label[0]] for label in train_labels])\n",
        "test_labels = tf.keras.utils.to_categorical([label_dict[label[0]] for label in test_labels])\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Define your data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "datagen.fit(train_images)\n",
        "\n",
        "training_data = datagen.flow(train_images, train_labels, batch_size=64)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ybvXqHzKdLdt"
      },
      "source": [
        "### Baseline CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Conv2D, Dense, Flatten, Dropout\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from keras.optimizers import Adam, SGD\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Define a function to create a Keras neural network model\n",
        "# def create_model(hidden_layers=1, neurons=16, optimizer=\"adam\"):\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    \n",
        "#     for _ in range(hidden_layers - 1):\n",
        "#         model.add(Conv2D(neurons, (3, 3), activation='relu'))\n",
        "#         model.add(Dropout(0.2))\n",
        "    \n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(128, activation='relu'))\n",
        "#     model.add(Dropout(0.5))\n",
        "#     model.add(Dense(4, activation='softmax'))\n",
        "        \n",
        "#     model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "#     return model\n",
        "\n",
        "# keras_clf = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# param_grid = {\n",
        "#     \"hidden_layers\": [2, 3],\n",
        "#     \"neurons\": [16, 32],\n",
        "#     'optimizer': ['adam', 'sgd'],\n",
        "#     \"batch_size\": [20, 40],\n",
        "#     \"epochs\": [10, 20],\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(estimator=keras_clf, param_grid=param_grid, cv=3, n_jobs=1)\n",
        "\n",
        "# # Fit the best estimator on the entire training dataset\n",
        "# best_estimator = grid_search.best_estimator_\n",
        "# best_estimator.fit(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "IE9N4RCqdRzd",
        "outputId": "a172caaf-e9a7-4eaf-9618-a66cd3a44efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.9773 - accuracy: 0.5407 - val_loss: 0.7213 - val_accuracy: 0.6805\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.7510 - accuracy: 0.6622 - val_loss: 0.6224 - val_accuracy: 0.7228\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.6696 - accuracy: 0.7088 - val_loss: 0.5408 - val_accuracy: 0.7730\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.6364 - accuracy: 0.7254 - val_loss: 0.5253 - val_accuracy: 0.7825\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.6082 - accuracy: 0.7431 - val_loss: 0.4831 - val_accuracy: 0.7977\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.5809 - accuracy: 0.7552 - val_loss: 0.5031 - val_accuracy: 0.7793\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.5569 - accuracy: 0.7684 - val_loss: 0.4700 - val_accuracy: 0.8120\n",
            "Epoch 8/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.5446 - accuracy: 0.7735 - val_loss: 0.4302 - val_accuracy: 0.8225\n",
            "Epoch 9/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.5204 - accuracy: 0.7815 - val_loss: 0.4953 - val_accuracy: 0.7935\n",
            "Epoch 10/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.5083 - accuracy: 0.7907 - val_loss: 0.4044 - val_accuracy: 0.8305\n",
            "Epoch 11/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.5048 - accuracy: 0.7878 - val_loss: 0.4038 - val_accuracy: 0.8360\n",
            "Epoch 12/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.4858 - accuracy: 0.7983 - val_loss: 0.4004 - val_accuracy: 0.8303\n",
            "Epoch 13/50\n",
            "313/313 [==============================] - 18s 59ms/step - loss: 0.4731 - accuracy: 0.8037 - val_loss: 0.3834 - val_accuracy: 0.8425\n",
            "Epoch 14/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.4705 - accuracy: 0.8054 - val_loss: 0.3886 - val_accuracy: 0.8380\n",
            "Epoch 15/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4601 - accuracy: 0.8050 - val_loss: 0.3771 - val_accuracy: 0.8415\n",
            "Epoch 16/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4545 - accuracy: 0.8127 - val_loss: 0.4319 - val_accuracy: 0.8235\n",
            "Epoch 17/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4490 - accuracy: 0.8130 - val_loss: 0.4225 - val_accuracy: 0.8288\n",
            "Epoch 18/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.4441 - accuracy: 0.8166 - val_loss: 0.3640 - val_accuracy: 0.8505\n",
            "Epoch 19/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.4428 - accuracy: 0.8154 - val_loss: 0.3643 - val_accuracy: 0.8518\n",
            "Epoch 20/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.4301 - accuracy: 0.8210 - val_loss: 0.3692 - val_accuracy: 0.8468\n",
            "Epoch 21/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.4306 - accuracy: 0.8229 - val_loss: 0.3831 - val_accuracy: 0.8418\n",
            "Epoch 22/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.4211 - accuracy: 0.8256 - val_loss: 0.3526 - val_accuracy: 0.8575\n",
            "Epoch 23/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.4123 - accuracy: 0.8274 - val_loss: 0.3614 - val_accuracy: 0.8540\n",
            "Epoch 24/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.4086 - accuracy: 0.8256 - val_loss: 0.4227 - val_accuracy: 0.8290\n",
            "Epoch 25/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3998 - accuracy: 0.8339 - val_loss: 0.3531 - val_accuracy: 0.8540\n",
            "Epoch 26/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4004 - accuracy: 0.8338 - val_loss: 0.3561 - val_accuracy: 0.8565\n",
            "Epoch 27/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3939 - accuracy: 0.8357 - val_loss: 0.3512 - val_accuracy: 0.8597\n",
            "Epoch 28/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.4067 - accuracy: 0.8327 - val_loss: 0.3691 - val_accuracy: 0.8568\n",
            "Epoch 29/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3918 - accuracy: 0.8393 - val_loss: 0.3501 - val_accuracy: 0.8543\n",
            "Epoch 30/50\n",
            "313/313 [==============================] - 19s 59ms/step - loss: 0.3897 - accuracy: 0.8404 - val_loss: 0.3457 - val_accuracy: 0.8635\n",
            "Epoch 31/50\n",
            "313/313 [==============================] - 19s 59ms/step - loss: 0.3858 - accuracy: 0.8402 - val_loss: 0.3600 - val_accuracy: 0.8500\n",
            "Epoch 32/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3842 - accuracy: 0.8417 - val_loss: 0.3424 - val_accuracy: 0.8620\n",
            "Epoch 33/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3770 - accuracy: 0.8432 - val_loss: 0.3334 - val_accuracy: 0.8625\n",
            "Epoch 34/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3779 - accuracy: 0.8439 - val_loss: 0.3477 - val_accuracy: 0.8587\n",
            "Epoch 35/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3760 - accuracy: 0.8442 - val_loss: 0.3498 - val_accuracy: 0.8575\n",
            "Epoch 36/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3767 - accuracy: 0.8474 - val_loss: 0.3304 - val_accuracy: 0.8708\n",
            "Epoch 37/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3693 - accuracy: 0.8483 - val_loss: 0.3390 - val_accuracy: 0.8637\n",
            "Epoch 38/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3672 - accuracy: 0.8496 - val_loss: 0.3217 - val_accuracy: 0.8695\n",
            "Epoch 39/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3642 - accuracy: 0.8494 - val_loss: 0.3336 - val_accuracy: 0.8677\n",
            "Epoch 40/50\n",
            "313/313 [==============================] - 18s 59ms/step - loss: 0.3624 - accuracy: 0.8496 - val_loss: 0.3307 - val_accuracy: 0.8708\n",
            "Epoch 41/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3591 - accuracy: 0.8514 - val_loss: 0.3228 - val_accuracy: 0.8780\n",
            "Epoch 42/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3520 - accuracy: 0.8567 - val_loss: 0.3422 - val_accuracy: 0.8683\n",
            "Epoch 43/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3594 - accuracy: 0.8520 - val_loss: 0.3212 - val_accuracy: 0.8742\n",
            "Epoch 44/50\n",
            "313/313 [==============================] - 19s 59ms/step - loss: 0.3527 - accuracy: 0.8589 - val_loss: 0.3542 - val_accuracy: 0.8687\n",
            "Epoch 45/50\n",
            "313/313 [==============================] - 18s 58ms/step - loss: 0.3529 - accuracy: 0.8565 - val_loss: 0.3546 - val_accuracy: 0.8600\n",
            "Epoch 46/50\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 0.3482 - accuracy: 0.8573 - val_loss: 0.3412 - val_accuracy: 0.8695\n",
            "Epoch 47/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.3399 - accuracy: 0.8598 - val_loss: 0.3539 - val_accuracy: 0.8643\n",
            "Epoch 48/50\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 0.3477 - accuracy: 0.8582 - val_loss: 0.3598 - val_accuracy: 0.8648\n",
            "Epoch 49/50\n",
            "313/313 [==============================] - 17s 56ms/step - loss: 0.3386 - accuracy: 0.8627 - val_loss: 0.3682 - val_accuracy: 0.8533\n",
            "Epoch 50/50\n",
            "313/313 [==============================] - 17s 56ms/step - loss: 0.3407 - accuracy: 0.8585 - val_loss: 0.3181 - val_accuracy: 0.8670\n"
          ]
        }
      ],
      "source": [
        "# Model architecture\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5), \n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using data augmentation generator\n",
        "history = cnn_model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                        epochs=50, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Save model\n",
        "cnn_model.save('cnn.h5')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 1s 9ms/step\n",
            "CNN Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.81      0.78      1000\n",
            "           1       0.83      0.74      0.78      1000\n",
            "           2       0.92      0.95      0.94      1000\n",
            "           3       0.96      0.97      0.96      1000\n",
            "\n",
            "    accuracy                           0.87      4000\n",
            "   macro avg       0.87      0.87      0.87      4000\n",
            "weighted avg       0.87      0.87      0.87      4000\n",
            "\n",
            "AUC: 0.9771002499999999\n",
            "CNN Confusion Matrix:\n",
            " [[806 138  39  17]\n",
            " [220 738  33   9]\n",
            " [ 22   7 952  19]\n",
            " [  8   8  12 972]]\n"
          ]
        }
      ],
      "source": [
        "# First, get the probabilities\n",
        "test_pred_probs = cnn_model.predict(test_images)\n",
        "\n",
        "# Then, convert the probabilities to class labels\n",
        "test_pred_classes = argmax(test_pred_probs, axis=1)\n",
        "\n",
        "# Getting the classification report\n",
        "print(\"CNN Classification Report:\\n\", classification_report(argmax(test_labels, axis=1), test_pred_classes))  # Convert one-hot to integers\n",
        "\n",
        "# Calculating ROC AUC\n",
        "roc_auc = roc_auc_score(argmax(test_labels, axis=1), test_pred_probs, multi_class='ovr')\n",
        "print(\"CNN AUC:\", roc_auc)\n",
        "\n",
        "# Getting the confusion matrix\n",
        "conf_mat = confusion_matrix(argmax(test_labels, axis=1), test_pred_classes)\n",
        "print(\"CNN Confusion Matrix:\\n\", conf_mat)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MjWrAsindWlo"
      },
      "source": [
        "### Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6B1ycYRdFvQ",
        "outputId": "6e120892-d053-4795-ceff-39e7e4c784d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 22s 81ms/step - loss: 1.2797 - accuracy: 0.4127 - val_loss: 1.1553 - val_accuracy: 0.5157\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 1.1322 - accuracy: 0.4951 - val_loss: 1.0960 - val_accuracy: 0.5132\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 1.0872 - accuracy: 0.5171 - val_loss: 1.0593 - val_accuracy: 0.5343\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 1.0542 - accuracy: 0.5368 - val_loss: 1.0420 - val_accuracy: 0.5443\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 1.0347 - accuracy: 0.5479 - val_loss: 1.0119 - val_accuracy: 0.5567\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 1.0160 - accuracy: 0.5571 - val_loss: 1.0014 - val_accuracy: 0.5617\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 1.0061 - accuracy: 0.5665 - val_loss: 0.9797 - val_accuracy: 0.5840\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.9986 - accuracy: 0.5641 - val_loss: 1.0240 - val_accuracy: 0.5415\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.9852 - accuracy: 0.5744 - val_loss: 0.9876 - val_accuracy: 0.5515\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.9749 - accuracy: 0.5768 - val_loss: 0.9979 - val_accuracy: 0.5537\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1ba03484490>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model architecture\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Add a new top layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "res_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# first, we only train the top layers (which were randomly initialized)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "res_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "res_model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Save model\n",
        "res_model.save('resnet.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzjwJxjyhvSN",
        "outputId": "f0ec9fdf-7590-4532-a3ac-84d3794ae4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 6s 46ms/step\n",
            "Resnet Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.04      0.08      1000\n",
            "           1       0.43      0.87      0.57      1000\n",
            "           2       0.71      0.65      0.68      1000\n",
            "           3       0.67      0.66      0.67      1000\n",
            "\n",
            "    accuracy                           0.56      4000\n",
            "   macro avg       0.59      0.55      0.50      4000\n",
            "weighted avg       0.59      0.56      0.50      4000\n",
            "\n",
            "AUC: 0.8436456666666666\n"
          ]
        }
      ],
      "source": [
        "# First, get the probabilities\n",
        "test_pred_probs = res_model.predict(test_images)\n",
        "\n",
        "# Then, convert the probabilities to class labels\n",
        "test_pred_classes = argmax(test_pred_probs, axis=1)\n",
        "\n",
        "# Getting the classification report\n",
        "print(\"Resnet Classification Report:\\n\", classification_report(argmax(test_labels, axis=1), test_pred_classes))  # Convert one-hot to integers\n",
        "\n",
        "# Calculating ROC AUC\n",
        "roc_auc = roc_auc_score(argmax(test_labels, axis=1), test_pred_probs, multi_class='ovr')\n",
        "print(\"AUC:\", roc_auc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.9582 - accuracy: 0.5610 - val_loss: 0.7919 - val_accuracy: 0.6320\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.7541 - accuracy: 0.6608 - val_loss: 0.6943 - val_accuracy: 0.6925\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6887 - accuracy: 0.6951 - val_loss: 0.6926 - val_accuracy: 0.6982\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6450 - accuracy: 0.7133 - val_loss: 0.6568 - val_accuracy: 0.7165\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6092 - accuracy: 0.7326 - val_loss: 0.6296 - val_accuracy: 0.7305\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5833 - accuracy: 0.7416 - val_loss: 0.6175 - val_accuracy: 0.7330\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5736 - accuracy: 0.7503 - val_loss: 0.5958 - val_accuracy: 0.7442\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5388 - accuracy: 0.7644 - val_loss: 0.6013 - val_accuracy: 0.7435\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5191 - accuracy: 0.7733 - val_loss: 0.5967 - val_accuracy: 0.7498\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4926 - accuracy: 0.7849 - val_loss: 0.5925 - val_accuracy: 0.7538\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1ba25e66f10>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Model architecture\n",
        "lenet_model = Sequential([\n",
        "    Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 3)),\n",
        "    AveragePooling2D(),\n",
        "    Conv2D(16, (5, 5), activation='relu'),\n",
        "    AveragePooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(120, activation='relu'),\n",
        "    Dense(84, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lenet_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),  # We are not using logits but softmax in the last layer\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lenet_model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Save model\n",
        "lenet_model.save('lenet.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 0s 1ms/step\n",
            "LeNet 5 Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.58      0.60      1000\n",
            "           1       0.65      0.68      0.66      1000\n",
            "           2       0.83      0.84      0.83      1000\n",
            "           3       0.88      0.87      0.87      1000\n",
            "\n",
            "    accuracy                           0.74      4000\n",
            "   macro avg       0.74      0.74      0.74      4000\n",
            "weighted avg       0.74      0.74      0.74      4000\n",
            "\n",
            "AUC: 0.9290319166666666\n"
          ]
        }
      ],
      "source": [
        "# First, get the probabilities\n",
        "test_pred_probs = lenet_model.predict(test_images)\n",
        "\n",
        "# Then, convert the probabilities to class labels\n",
        "test_pred_classes = argmax(test_pred_probs, axis=1)\n",
        "\n",
        "# Getting the classification report\n",
        "print(\"LeNet 5 Classification Report:\\n\", classification_report(argmax(test_labels, axis=1), test_pred_classes))  # Convert one-hot to integers\n",
        "\n",
        "# Calculating ROC AUC\n",
        "roc_auc = roc_auc_score(argmax(test_labels, axis=1), test_pred_probs, multi_class='ovr')\n",
        "print(\"AUC:\", roc_auc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tS228U9Ilqfw"
      },
      "source": [
        "### VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDeaKk-Tls_a",
        "outputId": "2a26a1ea-b72e-4a23-ba5c-fac5f6074c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 90s 355ms/step - loss: 1.3966 - accuracy: 0.2539 - val_loss: 1.3807 - val_accuracy: 0.2475\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 87s 347ms/step - loss: 1.3869 - accuracy: 0.2533 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 88s 352ms/step - loss: 1.3865 - accuracy: 0.2469 - val_loss: 1.3856 - val_accuracy: 0.2475\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 88s 350ms/step - loss: 1.3866 - accuracy: 0.2479 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 88s 352ms/step - loss: 1.3864 - accuracy: 0.2498 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 88s 352ms/step - loss: 1.3864 - accuracy: 0.2467 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 88s 351ms/step - loss: 1.3864 - accuracy: 0.2455 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 89s 356ms/step - loss: 1.3864 - accuracy: 0.2486 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 88s 354ms/step - loss: 1.3864 - accuracy: 0.2462 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 87s 347ms/step - loss: 1.3864 - accuracy: 0.2479 - val_loss: 1.3863 - val_accuracy: 0.2475\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1ba0d2c8e90>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vgg_model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(4, activation='softmax')  # 4 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "vgg_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "vgg_model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Save model\n",
        "vgg_model.save('vgg.h5')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 4s 34ms/step\n",
            "Resnet Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1000\n",
            "           1       0.00      0.00      0.00      1000\n",
            "           2       0.00      0.00      0.00      1000\n",
            "           3       0.25      1.00      0.40      1000\n",
            "\n",
            "    accuracy                           0.25      4000\n",
            "   macro avg       0.06      0.25      0.10      4000\n",
            "weighted avg       0.06      0.25      0.10      4000\n",
            "\n",
            "AUC: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\30640\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\30640\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\30640\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# First, get the probabilities\n",
        "test_pred_probs = vgg_model.predict(test_images)\n",
        "\n",
        "# Then, convert the probabilities to class labels\n",
        "test_pred_classes = argmax(test_pred_probs, axis=1)\n",
        "\n",
        "# Getting the classification report\n",
        "print(\"Resnet Classification Report:\\n\", classification_report(argmax(test_labels, axis=1), test_pred_classes))  # Convert one-hot to integers\n",
        "\n",
        "# Calculating ROC AUC\n",
        "roc_auc = roc_auc_score(argmax(test_labels, axis=1), test_pred_probs, multi_class='ovr')\n",
        "print(\"AUC:\", roc_auc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "LZ0b-k1nwy0V",
        "outputId": "a720b3e3-0d21-458e-b52d-bf34a2f886aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\30640\\AppData\\Local\\Temp\\ipykernel_18304\\2924262900.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=3)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n",
            "Best: 0.789101 using {'batch_size': 60, 'epochs': 10}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(4)  # 4 classes\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=3)\n",
        "\n",
        "# define the grid search parameters\n",
        "batch_size = [20, 60]\n",
        "epochs = [10, 50]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train_images, train_labels)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "# Predicting the classes\n",
        "test_pred = grid.predict(test_images)\n",
        "\n",
        "# Getting the classification report\n",
        "print(\"Classification Report:\\n\", classification_report(argmax(test_labels, axis=1), test_pred))  # Convert one-hot to integers\n",
        "\n",
        "# Calculating ROC AUC\n",
        "roc_auc = roc_auc_score(argmax(test_labels, axis=1), grid.predict_proba(test_images), multi_class='ovr')\n",
        "print(\"AUC:\", roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuned_cnn_test_loss, tuned_cnn_test_acc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "-uCZIIVOQNAo",
        "outputId": "4a86492c-989e-47d6-93b9-aadf5bd93eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "The image is classified as: Cat\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxA0lEQVR4nO3de3iU9bnv/89kkpmcJwTIyQTKQUHlYKWA2VaKQgXWrguVn9XqbxetS5ca3FVqD3S3nla7Yu1a1daL4tp7WegJbe0S3bparaKEWgEFoYi2KdBYUJIggRwYkkky8/z+8EdWo6DfGxK+JL5f1zXXRTIf7nyfeWbmzpN55p5QEASBAAA4wdJ8LwAA8NFEAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeJHuewHvlUqltGfPHuXl5SkUCvleDgDAKAgCtbW1qaysTGlpRz/OOeka0J49e1RRUeF7GQCA47R7926Vl5cf9fp+a0BLly7Vd7/7XTU0NGjy5Ml64IEHNG3atA/9f3l5eZKkP2+rUV5ertPPWv6jnzqva2/jfuesJO15u8E5O7yk2FQ7HHL/C2h3d7epdjLV5ZydcvbZptqhcMqUb2xwvw3LS0pMtfceOOCc3ffOO6ba5aXDnbPZkaipdmZmpilfX1/vvpZsW+3Va7c4Z6uX3WuqHaQynLMH2w6aar/66mvO2ReeedFU+8wJ40z5t99qdM5eetnlptoWoTTbU3oqlXDOXn/tQkPdlHbt2tXzfH40/dKAfvGLX2jx4sV68MEHNX36dN1///2aM2eOamtrVVRU9IH/9/Cf3fLycpWf79aAMjPdH/zRaMQ5K0kZGe4PoGjEVtvSgMIfcBh7JElDj8jKsj1hpRkbkGX/ZBvXktXef/s+y7LuqG3d1gZkWYslK0kZGe5PA3mOj8nDgqTtNrfIzs5yzkaMj03r/olG3W/znFzbbWhhbkBJ9+e3D/pT2tF82Mso/XISwve+9z1dd911uuaaa3TGGWfowQcfVHZ2tn70ox/1x48DAAxAfd6AOjs7tWnTJs2ePfu/fkhammbPnq1169a9L59IJNTa2trrAgAY/Pq8Ae3bt0/JZFLFxb1fDykuLlbDEV4LqK6uViwW67lwAgIAfDR4fx/QkiVL1NLS0nPZvXu37yUBAE6APj8JYdiwYQqHw2ps7H1WSGNjo0qOcIZTNBo1vYAHABgc+vwIKBKJaMqUKVq9enXP91KplFavXq3Kysq+/nEAgAGqX07DXrx4sRYuXKhPfOITmjZtmu6//37F43Fdc801/fHjAAADUL80oMsvv1zvvPOObr/9djU0NOiss87S008//b4TEwAAH12hIAgC34v4W62trYrFYsoNSa6j4PYe2O5c/1ePP2NaT1HhUOfsGWecYaq9detW52zFKUcfZ3Ekw4s/+A2/f+sd44SA2jfcb29JqvjYKc7ZM8+0vQM9Ho87Z7s6O021LQ8N6xsdu4yTLZr2uU8JmH/ZVabaI8Luv4f+229+aapdWFjonH3xxfe/TeOD5Ga7Pzbv/Pa/mmpf+/krTPnJk6Y4Z1PGN4v2p1TK/X44Y7r7NgYKlJTU0tKi/Pz8o+a8nwUHAPhoogEBALygAQEAvKABAQC8oAEBALygAQEAvKABAQC8oAEBALygAQEAvKABAQC8OHlmQrzHtVX/6PwxDZddfpNz3X/74T+b1rFs6S+cswfb2k21z54y2Tm7cbP72B5JSqW7/25RUDjEVHvUmNGmfHaO+8dt1NXVmWpbvLb1z6b81HM/7pwNKctUO5XmOGfq/5dIJJyzY0aNMNX+2c9WOGe/cvu9ptqWcUbzzp9qqv3Ms5uds5d/dr6pdigtacpHczOds/PnX2KqvXLlSudsOBw21U4zHIMML33/x+kcTSqVet9H8hz55wMA4AENCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgxUk7C27ypE8oOzvbKZtfWOhc97LP32Jax7f/163O2ayCiKn2C8+vdc6e9fGJptpra37vnB09ZqSpdsmwoab8mjW/c852d5tKa/bcOc7Z8vJyU+1Eh/s8sOws22w3ddtmjQ0d6n6bz5w501T7cwsvc84uuOwaU+2LF7jPYFv+4I9MtffUv+WcnTr1TFPtzkTclP/sZ/67e9j4rJsWyjCkU6baYcNaQhnu4VDKbR0cAQEAvKABAQC8oAEBALygAQEAvKABAQC8oAEBALygAQEAvKABAQC8oAEBALygAQEAvAgFQRD4XsTfam1tVSwW06ixZyktHHb6P5ZNCIVsoyoUuI9MObfSNi7nn+/8onP2gktuNNVOpdzXnZFhGfUhhY03YVrUvX44LdO4Frf7iCSlpdl+30qT+3idcLptDFM43XYjHow3O2f/15duMtXe+efNztlPfcZ9bI8kPfnYL52zDXs7TLU/ec5U52wylTDVVpdtVNI/f+f7ztk3395tqv3SSy+Z8ha5ue6Pt4qKCudsa2uryktK1dLSovz8/KPmOAICAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeJHuewFHc8eSG5WdneWULcjJc64b77TNmwqF3GdC7a7fa6r9tW8+4Jz99cP3mWo/tPxXztn/57JLTLW3bdtmypeWlDtnc3JyTLUzM91nWRUVFZlqP/fcc87ZT1/wKVPtcMTtvn3Yf/zyYedsKmWbM1c2epxz9i912021y0eOcc427q011Y5mus8B7Dpk+127S7ZZcJZ5lP05282qePhw5+zeve7Pb21tbU45joAAAF70eQO68847FQqFel3Gjx/f1z8GADDA9cuf4M4888xef75ITz9p/9IHAPCkXzpDenq6SkpK+qM0AGCQ6JfXgLZv366ysjKNHj1aV111lXbt2nXUbCKRUGtra68LAGDw6/MGNH36dK1YsUJPP/20li1bprq6Op133nlHPSuiurpasVis52L51D0AwMDV5w1o3rx5uuyyyzRp0iTNmTNHv/71r9Xc3Kxf/vLIH827ZMkStbS09Fx277Z9XC0AYGDq97MDCgoKdNppp2nHjh1HvD4ajSoajfb3MgAAJ5l+fx/QwYMHtXPnTpWWlvb3jwIADCB93oBuu+021dTU6M0339RLL72kSy65ROFwWJ/73Of6+kcBAAawPv8T3FtvvaXPfe5zampq0vDhw/XJT35S69ev13DDyAdJ+vY9/6Jw2G3URsgw1iQUuI/vkKSw4Rbq6uoy1nb/0+MlC28x1f6nW//ROZsdto2FmXrWVFO+td39zMb8vCGm2s3xZvdss3tWki6Y82nnbJDqNtVWWrspbjk5Jwi5jyeSpLaD7vtnRMUoU+3RI85wzna099/4G9fnkp61dNhGdlkkk7bttK7d4szxk5yzlrFXruOg+rwBPfLII31dEgAwCDELDgDgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgRb9/HMOxGjOyQhkZbsvbv3+/c9229k7TOroT7jO+OrpttaMhw/ww2+gwff2e/+2cLRiSa6odbzfOPTMIyTb3KhKJ9EtWkrKy3Wf1pdJtv8tF07JN+U9PNMxgy7PNJAyFi5yzu95801R782s1ztlzzhpjqm2ZkRaE3efGSVJLp+0B97NfPGzKWwQp9/vWX+q2m2pbHhNr1651zra1tWnCxPEfmuMICADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgxUk7iue2Ly9WTk6OU/a1rduc6za1NpvWkZ3tPjKlu9s2oiYtzb3/h0IhU22LILCNKenqso16icfjztnOTts4o7y8POdsNOo+WkeS0tPdR73c8PGEqXYymTTlwznu27n9j2+aasfT9jln9xXbxuXMHOo+QigatJlqpxLu98NOw9geSZo6daopb3kMWR8/lglSW7duNdW2rDs3M8s5m3LcRo6AAABe0IAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXtCAAABe0IAAAF6ctLPgIpGwolG3+U2TzzrDuW7CNoJL+/cfcM4eOnTIVDsjI8M5GzbOssrKcp/bZJmnJknJpG3m3V//uss529TUZKqdmZlpyNpmweXm5jpnO9N2m2pHjbfh4u8+7pz94v8431Q7lHQfNvZ8rfvcOEmq/Hi+czY7UmSqfWCv+1qGRG2/a2dnuq9bkqI57vnW1lZTbYu///u/N+V//OMfO2enTvu4czaVSjnlOAICAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeHHSzoJ7du0G5zlfljlpiUTCtI533nnHOZuRtA2aC2W43/yhUMhUOxzuvzlzyW7j3SbDfe5ZKNVpKp0edp8Fl0i0m2ofigfO2fn/OMZU+8vf+g9Tfsv6Pzhn4xedZ6pdMPJjztlpw8eZag/Jcb+vFBYUmGpHQob7bbLDVDvRbZvVd9899zhnb7rpJlPttDT34wT3e+y7csLuzyvf/vY/O2cPHTqka677hw/NcQQEAPDC3IDWrl2riy66SGVlZQqFQnr88cd7XR8EgW6//XaVlpYqKytLs2fP1vbt2/tqvQCAQcLcgOLxuCZPnqylS5ce8fp7771XP/jBD/Tggw9qw4YNysnJ0Zw5c9TRYTsEBgAMbubXgObNm6d58+Yd8bogCHT//ffrG9/4hubPny9J+slPfqLi4mI9/vjjuuKKK45vtQCAQaNPXwOqq6tTQ0ODZs+e3fO9WCym6dOna926dUf8P4lEQq2trb0uAIDBr08bUENDgySpuLi41/eLi4t7rnuv6upqxWKxnktFRUVfLgkAcJLyfhbckiVL1NLS0nPZvdv20cYAgIGpTxtQSUmJJKmxsbHX9xsbG3uue69oNKr8/PxeFwDA4NenDWjUqFEqKSnR6tWre77X2tqqDRs2qLKysi9/FABggDOfBXfw4EHt2LGj5+u6ujpt2bJFhYWFGjFihG655RZ961vf0qmnnqpRo0bpm9/8psrKynTxxRf35boBAAOcuQFt3LhR559/fs/XixcvliQtXLhQK1as0Fe+8hXF43Fdf/31am5u1ic/+Uk9/fTTzmN1DisZNlxZWVlO2SAcca7bcbDFtI7MDPdRFV1dXabalhE41tq5ubnO2VQqZaptGQ1irR8EtmEiZ4+f5JzdsekpU+3GkPtt/j+++ICp9g3X3WbKl5S84Zz9zK0/NNX+vz/7nnO2YvRQU+2w3G/DAuMoHgXuj820wDbi6e0G2/OEZbxOWprt8Zab7v40/fwLz5hq3/blLzpnW1r3OWfb293GXpkb0MyZMz/wSSIUCunuu+/W3XffbS0NAPgI8X4WHADgo4kGBADwggYEAPCCBgQA8IIGBADwggYEAPCCBgQA8IIGBADwggYEAPCCBgQA8MI8iudEycgIKSPiNuupq+uQc920sG0OU3d3t3PWOu/OMlMtK9193p0k5eTlOWc7425zmw6zfmTG/v37nbNBhu13oqFR95ldP1u7xVT70otnOWcLu0411X557X+a8gcT7vtoXEWOqfbczy5yzr6yeYupdipwf7xZHmuSlBFxf/qKRLJNtWtrXjLlE50dztk8w2NTencAtKtIxPY8MXxYkXM2Ho87Z113O0dAAAAvaEAAAC9oQAAAL2hAAAAvaEAAAC9oQAAAL2hAAAAvaEAAAC9oQAAAL2hAAAAvTtpRPJnpGc7jZ5LJpHPdzs7OY13Sh7KOEsnOiDpnA7epRP+V73RfS3a2bUxJR4f72BFJysrKcs4e6kqYaufkuo96mTZpmKl2LL/QOTvh9NGm2gf2u4+PkqS1r9Y6Z4cPKTbVDpJNztmLP/N3ptq/euJR5+y+fftMtS33q44O93E2kpQwjJ2RpOws9/E6hulEkqT2dvcxTNZRPJbxOi0t7mOv2h2fIzgCAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhx0s6CS6UCpVJuQ5Ni2bnOdZMdtllwlnwoZBvYlpGR4Zx1vS0OC4LAOduRcJ81JUkjKkaa8vv373fORqPu8/EkSbljnaPzbviWqfSerb93zqYXTDTVLh2dacpv2Oo+C27O+dNNtYtKi5yzNVveMNXuPOR+3/rLX/eYan/sVPf5e6XD3bdRss+MDIfdZ/ulddp+708Puz9PhFO256BEwn32Ynq6e7tID7tlOQICAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhx0o7iycxKU2ZW2CnbYRiXYx31kkwmnbOW0TqSFIlEnLPd3d2m2rm57uOJAtnG/CQ6O0z5tLD7eJC0sNs+P2zza9uds2dNHG+qPXri2c7ZHRv/01R77e/+bMrPOu+Tztn/++xaU+3tu+qdswvn/DdT7Qe+XuWcLS0bbqr9u3jcOXvKCPexPZIUitpGJXVE3O9b3V3uzynv/gf3fDxlq51j2M5Eos0529nh9hzBERAAwAsaEADAC3MDWrt2rS666CKVlZUpFArp8ccf73X91VdfrVAo1Osyd+7cvlovAGCQMDegeDyuyZMna+nSpUfNzJ07V/X19T2Xhx9++LgWCQAYfMwnIcybN0/z5s37wEw0GlVJSckxLwoAMPj1y2tAa9asUVFRkcaNG6cbb7xRTU1NR80mEgm1trb2ugAABr8+b0Bz587VT37yE61evVrf+c53VFNTo3nz5h31dObq6mrFYrGeS0VFRV8vCQBwEurz9wFdccUVPf+eOHGiJk2apDFjxmjNmjWaNWvW+/JLlizR4sWLe75ubW2lCQHAR0C/n4Y9evRoDRs2TDt27Dji9dFoVPn5+b0uAIDBr98b0FtvvaWmpiaVlpb2948CAAwg5j/BHTx4sNfRTF1dnbZs2aLCwkIVFhbqrrvu0oIFC1RSUqKdO3fqK1/5isaOHas5c+b06cIBAAObuQFt3LhR559/fs/Xh1+/WbhwoZYtW6atW7fqxz/+sZqbm1VWVqYLL7xQ//RP/2SewZZIdCrsPBcscK5rnXuWnn7Sjsv7QO0dh5yzaWm2A+GCggJTPhRynwVnmb0nSdGI+yyr9oPNptrbX1zhnM0K2+YADi8Zasp3h9wfP03NtjNJh8Tc/+w9JMu2f8aNKXLOWs+APe/cac7ZVzZtM9Vub2835bt2uM8kHDrOfa6fZJtfGTLOmesMEs7ZVLf780Sqyy1rfnadOXOmguDoT/jPPPOMtSQA4COIWXAAAC9oQAAAL2hAAAAvaEAAAC9oQAAAL2hAAAAvaEAAAC9oQAAAL2hAAAAvaEAAAC9O2kFn7QdapY5Op2xnp1tOkjpT3aZ1RA0zvg4Z1iG9O9jVlXUmXU7+EOdsSUGeqXaeYXaYJKUMt/n5cz5rqn3q6BHO2V899F1T7cmV7//8qqN5/pm1ptrNnc2m/Jsvu88yKx5WaKodDbv/HtrwZp2p9pE+A+xofv/qK6baiUNx5+yokeWm2pHMHFM+Hndfy8YtNaba+aPOcs5aZxIOHeo+k7Ar2WXIuj3mOQICAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhx0o7i2VLXrGg06pTd3+I+BuNAm3tWso3AicViptqxnLBzNjs7ZKrdafjVYvO2P5pqZ2baxn1kGsaa/O9rrjTVHvnfP+Oc/dxNXzTVfuLfvuWcLR9VYqqdvT9pyr/9VqNz9i+7dplqDxsyzDlbOfMsU+0/7vizc3ZEycdMtfcecB9ltW/fAVPtPXv2mPJnn322c/a0saNNtWu273DOlgwdbqqdk+P+2Eym3O+zrlmOgAAAXtCAAABe0IAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXtCAAABenLSz4Pbvf1uRSMQp29HR4Vw31NVpWkfQ5X4THWhvMNVuNtz8CxcuNNVe+fBPnbPhNLfb+bBI1H2GndXrQcqU/4c/r3POfuuyS021177kPoOrcsoYU+2Sti5T/p199c7ZPMN8L0kqKR7qnN26ZbuptuWuVVBg2/dvvvmWc7b1YLupdnFxsSn/29U1ztmrvnCjqfb2Jve1dyVtMwabmpqcs3l5ec7ZVLfbvuQICADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgxUk7iqekIFvRaNQpGwT5hsrdpnXMmzfPORtKzzXVtkgkEqb8lVd+1jmblWEbxbP2+RdMeYvMTNtd8h//9cfO2fnTJ5pq7z/gPqbklDNuN9XubnjFlG9sbHTO7t7XYqo94xOnOGcrcspNtTdvrXXO5hvH35xSMtw5m2scxdPcYst/9tIFztmN639vqv1m7ZvO2Y+dNtlUuy1+0DkbCrsfr7g+X3EEBADwwtSAqqurNXXqVOXl5amoqEgXX3yxamt7/4bT0dGhqqoqDR06VLm5uVqwYIHptzcAwEeDqQHV1NSoqqpK69ev17PPPquuri5deOGFisfjPZlbb71VTz75pB599FHV1NRoz549uvRS2xRiAMDgZ/qD+9NPP93r6xUrVqioqEibNm3SjBkz1NLSooceekgrV67UBRdcIElavny5Tj/9dK1fv17nnHNO360cADCgHddrQC0t777YWVhYKEnatGmTurq6NHv27J7M+PHjNWLECK1bd+TPbUkkEmptbe11AQAMfsfcgFKplG655Rade+65mjBhgiSpoaFBkUhEBQUFvbLFxcVqaDjyh7VVV1crFov1XCoqKo51SQCAAeSYG1BVVZW2bdumRx555LgWsGTJErW0tPRcdu/efVz1AAADwzG9D2jRokV66qmntHbtWpWX/9f7AkpKStTZ2anm5uZeR0GNjY0qKSk5Yq1oNOr8fh8AwOBhOgIKgkCLFi3SqlWr9Pzzz2vUqFG9rp8yZYoyMjK0evXqnu/V1tZq165dqqys7JsVAwAGBdMRUFVVlVauXKknnnhCeXl5Pa/rxGIxZWVlKRaL6dprr9XixYtVWFio/Px83XzzzaqsrOQMOABAL6YGtGzZMknSzJkze31/+fLluvrqqyVJ9913n9LS0rRgwQIlEgnNmTNHP/zhD/tksQCAwcPUgIIg+NBMZmamli5dqqVLlx7zoiQpKyvL+bWhtLT+myg08RMznLN79+411c7MynHOdrTbZlO99offOWffqq831R4yJM+UT88IO2dnnjnTVLty1NnO2e/8H9t9siGR4Zyd906Hqfb48jGmvAL32XGL/uEyU+l4/V+cs5n5MVPtINf9vhLNLzTV7m51n484rKjAVHvM+CJT/pXXtrqHM2wzI6OZ7o+fcNg9K707ucZVKpUyZD+8V0jMggMAeEIDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeHFMH8dwIuTm5iozM9Mp2x1yG/sgSepKmtZx379UO2f/55duM9VOdrv3/7bWZlPtrHC+czY90mSqXZLrPkJIksJR97tZS1uzqfYf/lTrnC0rGGaqvXb1fzpnCwuHmGrnZrqPNZGkcz/lPhJq7OjRptrbDrU5Z599+llT7aHDTnHOBiHb09HRPuTySPKM+/71P7mPJ5Kk2p3bnbOnT7INZs7KHeqcjSdsI6GyIhHnbGdnp3u2yy3LERAAwAsaEADACxoQAMALGhAAwAsaEADACxoQAMALGhAAwAsaEADACxoQAMALGhAAwAsaEADAi5N2Flwo1alQKuSU7Yi7b0a87W3TOl79w6vO2WDxV0y1FXKfB9bd3W0rneF+mwTG+Wt7k3FTPj3sNtNPkt5o2mKqHd/f4pw9c0qpqfbobe7z3YZkHDDVzjDOPRt6SplztmG/+2w3SYpmFjpnL5xzian2M6ufc85u/YP7XD9JKjbcJqeMHGmq/buXN5nyqXCec/bN+npT7Z173Gc1Tjk7ZqqdlmaYBdfR5Z7tdMtyBAQA8IIGBADwggYEAPCCBgQA8IIGBADwggYEAPCCBgQA8IIGBADwggYEAPCCBgQA8OKkHcVzsK1N3Z2dbtn4Qee66em2TZ5xzn/rt9qdjtsnSZmZ7uNsJOmrX/uSc/aqyz5tqh2NuI/vkKTWdvfRMNbtTCt0G9ckSQXDh5pqf+Ksyc7Z3z7/e1Ptn/50pSn/D1+4zDl73f97ual2Z/xPztnn1qw11S4rHeGcjWbbxsj8/Ke/cs4OH77ZVHvuRXNM+ZfWu9f/S+12U+0hFac5Z8PhsKm2AsPILrmPAwuCwCnHERAAwAsaEADACxoQAMALGhAAwAsaEADACxoQAMALGhAAwAsaEADACxoQAMALGhAAwAsaEADAi5N2FlxmdkSZmW4zxza/fsC5bnl5rmkdB+MtztnaV5821c7MdZ999crLr5pqL7zxGudsVkezqXYqsM2bystwnyEVCrnPdpOkcCzHOdvY2GiqXbNxvXN2ytTxptr/8p27TfmNL29wzv4fw4w0SWrY/Vfn7OmjK0y19+5tcM5m5RWYap/xcfdZfQ0N75hqN7W6z2mUpN173nbOjj/746baJeXus+BaW1tNtZXu/tjs7u5wzya7nHIcAQEAvDA1oOrqak2dOlV5eXkqKirSxRdfrNra2l6ZmTNnKhQK9brccMMNfbpoAMDAZ2pANTU1qqqq0vr16/Xss8+qq6tLF154oeLxeK/cddddp/r6+p7Lvffe26eLBgAMfKbXgJ5+uvdrHCtWrFBRUZE2bdqkGTNm9Hw/OztbJSUlfbNCAMCgdFyvAbW0vPsCfWFhYa/v//znP9ewYcM0YcIELVmyRIcOHTpqjUQiodbW1l4XAMDgd8xnwaVSKd1yyy0699xzNWHChJ7vX3nllRo5cqTKysq0detWffWrX1Vtba0ee+yxI9aprq7WXXfddazLAAAMUMfcgKqqqrRt2za9+OKLvb5//fXX9/x74sSJKi0t1axZs7Rz506NGTPmfXWWLFmixYsX93zd2tqqigrbqZ4AgIHnmBrQokWL9NRTT2nt2rUqLy//wOz06dMlSTt27DhiA4pGo4pGo8eyDADAAGZqQEEQ6Oabb9aqVau0Zs0ajRo16kP/z5YtWyRJpaWlx7RAAMDgZGpAVVVVWrlypZ544gnl5eWpoeHddznHYjFlZWVp586dWrlypf7u7/5OQ4cO1datW3XrrbdqxowZmjRpUr9sAABgYDI1oGXLlkl6982mf2v58uW6+uqrFYlE9Nxzz+n+++9XPB5XRUWFFixYoG984xt9tmAAwOBg/hPcB6moqFBNTc1xLeiwPekViqRnOWXHjqo3VLadeR6E3NYgSf/x6OOm2mnp7jPVurrcZisdljjknh9z6ge/jvdeqVTKlLfIzbXN6jt06KBz1nobJhPus682bXjZVDuWkWnKT5vm/heE1EHbWxmy09qds9Ec2+u1LV1J9+xfdplqW5xzzlRT3vo8lpTb3EpJ2rRxq6n2pG73+YjjTnWfGyd9+HP632pLZDhn08JudZkFBwDwggYEAPCCBgQA8IIGBADwggYEAPCCBgQA8IIGBADwggYEAPCCBgQA8IIGBADw4pg/D6i/Rfb8QdGI23iLtAz3MRhK2npuxegRzlnLWAtJ6grc17Kv4W1TbSXdR9QkEglT6cxM2xiZcNh95JBkG/OTHriPKZFhLIwknTP9E87ZV17baapdX99oyi+46CLn7LCY7X5YXlrknN322p9MtUty3Me3FMdiptqxTPdRSaVF2abap59qm97f1Oq+lndabPfDktLhztm8vDxT7fbOuHM20/DQDDneBTkCAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhx0s6C60ollJZyGyiUGXKfTWabkiUlk+5zm3Jzc021QyH3OWZpw9znQUlSY9J9cFMkwzYnKxw2zF+TlEp1O2eDTlNpBV3utXMzo6bap1a4zwOLpGwL/8uu3ab8Y88975w9bexppto1v37MOfs/F1WZakcNYxpX/OTnptrFQ4c4Z+OyPTb/9Oc6U/7tA+6z4JpabLMXY4YZeclO2zOcZc5cZsT98RNynNHIERAAwAsaEADACxoQAMALGhAAwAsaEADACxoQAMALGhAAwAsaEADACxoQAMALGhAAwIuTdhRPblaOoo5zPLpT/TcuxzJ+YsyYMaba+/btc87G2w6aamdnu4/XycjIMNU+eLDZlLdUzzDc3lbRSI4pn5l5yDl71llnmWqXG8b8SFLTvmbn7BuvbzTVPhhyv83v/NcfmWq3Gp5iOlsPmGrnZrnnI1tstdsOtpvy4Uz30Vd5mcan3bB77US3bd1vv/22c3bo0KHO2e6k24gsjoAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXtCAAABe0IAAAF7QgAAAXpy0s+A6Eh0KArcZSKGwex8NpbnPVZKkRCLhnN39112m2hUjRzhnW1tbTbW7k+7rPhhvNtXODIdN+eyo+90slAxMtWMF+c7ZrsB9ZqAkpacVOGcPHLDNGktPuc3KOqykwH1eW2G2+/1KkkaVDHHOJlPu9ytJ2vtOm3O2vTPLVDvZ7f647+rqMtVub7fNJGyKu98usSEFptpKuT8mkknbfTxkeLjt29/knO10vL05AgIAeGFqQMuWLdOkSZOUn5+v/Px8VVZW6je/+U3P9R0dHaqqqtLQoUOVm5urBQsWqLGxsc8XDQAY+EwNqLy8XPfcc482bdqkjRs36oILLtD8+fP1+uuvS5JuvfVWPfnkk3r00UdVU1OjPXv26NJLL+2XhQMABjbTa0AXXXRRr6+//e1va9myZVq/fr3Ky8v10EMPaeXKlbrgggskScuXL9fpp5+u9evX65xzzum7VQMABrxjfg0omUzqkUceUTweV2VlpTZt2qSuri7Nnj27JzN+/HiNGDFC69atO2qdRCKh1tbWXhcAwOBnbkCvvfaacnNzFY1GdcMNN2jVqlU644wz1NDQoEgkooKCgl754uJiNTQ0HLVedXW1YrFYz6WiosK8EQCAgcfcgMaNG6ctW7Zow4YNuvHGG7Vw4UK98cYbx7yAJUuWqKWlpeeye/fuY64FABg4zO8DikQiGjt2rCRpypQpeuWVV/T9739fl19+uTo7O9Xc3NzrKKixsVElJSVHrReNRhWN2s65BwAMfMf9PqBUKqVEIqEpU6YoIyNDq1ev7rmutrZWu3btUmVl5fH+GADAIGM6AlqyZInmzZunESNGqK2tTStXrtSaNWv0zDPPKBaL6dprr9XixYtVWFio/Px83XzzzaqsrOQMOADA+5ga0N69e/X5z39e9fX1isVimjRpkp555hl9+tOfliTdd999SktL04IFC5RIJDRnzhz98Ic/PKaFtcc7lexymxORkZHhXjfcaVpHJKPDOWt9021Tk/toC+vZgWHDns3PyjHV7mo/ZMt3uo8/6k60m2pbRiWF021jfjri7tuZ7D5oqp3pfpeVJMXj7vfb7EimqXY43318S1Oz7TbMNmxot21ajg62u9/mHR22x30yZFtLLMf9Nre+5BA2jL4KAtv+STqOO5Ok7k73+0lXl9uoKVMDeuihhz7w+szMTC1dulRLly61lAUAfAQxCw4A4AUNCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOAFDQgA4AUNCADgBQ0IAOCFeRp2fzs8SqKzy30uh2X8RCJhG8nRYRj1khay9fP0dPfRFpZ1SFLYvbTS5T6OQ5K6jLdhetJ9Md3G2pYxJeGkcRSPYS2JTuMcGePcmUSn22gTSQqlbPfDhOPYFEnq7DLcsSR1dRtGvSRt98Nkyj2fTBlH1BhH8QRyr580bmd3d9+PwDksLc39vmJ5nj28jg/7P6HAOjyon7311lt8KB0ADAK7d+9WeXn5Ua8/6RpQKpXSnj17lJeXp1Dov34NaW1tVUVFhXbv3q38/HyPK+xfbOfg8VHYRontHGz6YjuDIFBbW5vKyso+8CjrpPsTXFpa2gd2zPz8/EG98w9jOwePj8I2SmznYHO82xmLxT40w0kIAAAvaEAAAC8GTAOKRqO64447zB/mNNCwnYPHR2EbJbZzsDmR23nSnYQAAPhoGDBHQACAwYUGBADwggYEAPCCBgQA8GLANKClS5fqYx/7mDIzMzV9+nS9/PLLvpfUp+68806FQqFel/Hjx/te1nFZu3atLrroIpWVlSkUCunxxx/vdX0QBLr99ttVWlqqrKwszZ49W9u3b/ez2OPwYdt59dVXv2/fzp07189ij1F1dbWmTp2qvLw8FRUV6eKLL1ZtbW2vTEdHh6qqqjR06FDl5uZqwYIFamxs9LTiY+OynTNnznzf/rzhhhs8rfjYLFu2TJMmTep5s2llZaV+85vf9Fx/ovblgGhAv/jFL7R48WLdcccdevXVVzV58mTNmTNHe/fu9b20PnXmmWeqvr6+5/Liiy/6XtJxicfjmjx5spYuXXrE6++991794Ac/0IMPPqgNGzYoJydHc+bMUUdHxwle6fH5sO2UpLlz5/batw8//PAJXOHxq6mpUVVVldavX69nn31WXV1duvDCCxWPx3syt956q5588kk9+uijqqmp0Z49e3TppZd6XLWdy3ZK0nXXXddrf957772eVnxsysvLdc8992jTpk3auHGjLrjgAs2fP1+vv/66pBO4L4MBYNq0aUFVVVXP18lkMigrKwuqq6s9rqpv3XHHHcHkyZN9L6PfSApWrVrV83UqlQpKSkqC7373uz3fa25uDqLRaPDwww97WGHfeO92BkEQLFy4MJg/f76X9fSXvXv3BpKCmpqaIAje3XcZGRnBo48+2pP54x//GEgK1q1b52uZx+292xkEQfCpT30q+OIXv+hvUf1kyJAhwb//+7+f0H150h8BdXZ2atOmTZo9e3bP99LS0jR79mytW7fO48r63vbt21VWVqbRo0frqquu0q5du3wvqd/U1dWpoaGh136NxWKaPn36oNuvkrRmzRoVFRVp3LhxuvHGG9XU1OR7ScelpaVFklRYWChJ2rRpk7q6unrtz/Hjx2vEiBEDen++dzsP+/nPf65hw4ZpwoQJWrJkiQ4dOuRjeX0imUzqkUceUTweV2Vl5QndlyfdMNL32rdvn5LJpIqLi3t9v7i4WH/60588rarvTZ8+XStWrNC4ceNUX1+vu+66S+edd562bdumvLw838vrcw0NDZJ0xP16+LrBYu7cubr00ks1atQo7dy5U1//+tc1b948rVu3zvR5RieLVCqlW265Reeee64mTJgg6d39GYlEVFBQ0Cs7kPfnkbZTkq688kqNHDlSZWVl2rp1q7761a+qtrZWjz32mMfV2r322muqrKxUR0eHcnNztWrVKp1xxhnasmXLCduXJ30D+qiYN29ez78nTZqk6dOna+TIkfrlL3+pa6+91uPKcLyuuOKKnn9PnDhRkyZN0pgxY7RmzRrNmjXL48qOTVVVlbZt2zbgX6P8MEfbzuuvv77n3xMnTlRpaalmzZqlnTt3asyYMSd6mcds3Lhx2rJli1paWvSrX/1KCxcuVE1NzQldw0n/J7hhw4YpHA6/7wyMxsZGlZSUeFpV/ysoKNBpp52mHTt2+F5Kvzi87z5q+1WSRo8erWHDhg3Ifbto0SI99dRTeuGFF3p9bEpJSYk6OzvV3NzcKz9Q9+fRtvNIpk+fLkkDbn9GIhGNHTtWU6ZMUXV1tSZPnqzvf//7J3RfnvQNKBKJaMqUKVq9enXP91KplFavXq3KykqPK+tfBw8e1M6dO1VaWup7Kf1i1KhRKikp6bVfW1tbtWHDhkG9X6V3P/W3qalpQO3bIAi0aNEirVq1Ss8//7xGjRrV6/opU6YoIyOj1/6sra3Vrl27BtT+/LDtPJItW7ZI0oDan0eSSqWUSCRO7L7s01Ma+skjjzwSRKPRYMWKFcEbb7wRXH/99UFBQUHQ0NDge2l95ktf+lKwZs2aoK6uLvj9738fzJ49Oxg2bFiwd+9e30s7Zm1tbcHmzZuDzZs3B5KC733ve8HmzZuDv/71r0EQBME999wTFBQUBE888USwdevWYP78+cGoUaOC9vZ2zyu3+aDtbGtrC2677bZg3bp1QV1dXfDcc88FZ599dnDqqacGHR0dvpfu7MYbbwxisViwZs2aoL6+vudy6NChnswNN9wQjBgxInj++eeDjRs3BpWVlUFlZaXHVdt92Hbu2LEjuPvuu4ONGzcGdXV1wRNPPBGMHj06mDFjhueV23zta18Lampqgrq6umDr1q3B1772tSAUCgW//e1vgyA4cftyQDSgIAiCBx54IBgxYkQQiUSCadOmBevXr/e9pD51+eWXB6WlpUEkEglOOeWU4PLLLw927Njhe1nH5YUXXggkve+ycOHCIAjePRX7m9/8ZlBcXBxEo9Fg1qxZQW1trd9FH4MP2s5Dhw4FF154YTB8+PAgIyMjGDlyZHDdddcNuF+ejrR9koLly5f3ZNrb24ObbropGDJkSJCdnR1ccsklQX19vb9FH4MP285du3YFM2bMCAoLC4NoNBqMHTs2+PKXvxy0tLT4XbjRF77whWDkyJFBJBIJhg8fHsyaNaun+QTBiduXfBwDAMCLk/41IADA4EQDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHhBAwIAeEEDAgB4QQMCAHjx/wFd0w//ajUBxAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the image\n",
        "img_path = 'cat2.jpg'  # replace with your image path\n",
        "img = image.load_img(img_path, target_size=(32, 32))\n",
        "\n",
        "# Convert the image to a numpy array\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# Normalize the image\n",
        "x = x / 255.0\n",
        "\n",
        "# Add a dimension to transform the array into a \"batch\" of size (1, 32, 32, 3)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Perform the prediction\n",
        "preds = model.predict(x)\n",
        "\n",
        "# Decode the prediction\n",
        "pred_class = np.argmax(preds[0])\n",
        "\n",
        "# Map the label index to the corresponding class\n",
        "label_map = {0: 'Cat', 1: 'Dog', 2: 'Airplane', 3: 'Automobile'}  # adjust this to your classes\n",
        "print(f'The image is classified as: {label_map[pred_class]}')\n",
        "\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "with CustomObjectScope({'dice_coef': dice_coef}):\n",
        "    model = load_model('cnn.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
