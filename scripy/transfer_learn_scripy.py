import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid
import src.relabel_data as rd
import cv2
import numpy as np
from src.create_augement import createAugment
import matplotlib.image as mpimg
from keras.utils import CustomObjectScope
from keras.models import load_model
from PIL import Image, ImageDraw
import random
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation
from keras.applications.vgg19 import VGG19
import json

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# get the new data after filtering and relabeling
x_train, y_train, x_test, y_test = rd.filter_relabel(x_train, y_train, x_test, y_test)

train_masked = createAugment(x_train, x_train)
test_masked = createAugment(x_test, x_test)

## Metric
def dice_coef(y_true, y_pred):
    y_true_f = keras.backend.flatten(y_true)
    y_pred_f = keras.backend.flatten(y_pred)
    intersection = keras.backend.sum(y_true_f * y_pred_f)
    return (2. * intersection + 1) / (keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) + 1)

def unet_vgg():
    base_model = VGG19(include_top=False, weights='imagenet', input_shape=(32, 32, 3))
    base_model.trainable = False  # Freeze the base_model

    # Collect encoder layers from VGG19
    conv1 = base_model.get_layer('block1_conv2').output
    conv2 = base_model.get_layer('block2_conv2').output
    conv3 = base_model.get_layer('block3_conv4').output

    # Bottleneck
    conv4 = keras.layers.Conv2D(256, (3, 3), activation=keras.layers.LeakyReLU(), padding='same')(conv3)
    conv4 = keras.layers.BatchNormalization()(conv4)
    conv4 = keras.layers.Dropout(0.1)(conv4)

    # Decoding / Upsampling
    up1 = keras.layers.concatenate([keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv4), conv2], axis=3)
    conv5 = keras.layers.Conv2D(128, (3, 3), activation=keras.layers.LeakyReLU(), padding='same')(up1)
    conv5 = keras.layers.BatchNormalization()(conv5)
    conv5 = keras.layers.Dropout(0.1)(conv5)

    up2 = keras.layers.concatenate([keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv5), conv1], axis=3)
    conv6 = keras.layers.Conv2D(64, (3, 3), activation=keras.layers.LeakyReLU(), padding='same')(up2)
    conv6 = keras.layers.BatchNormalization()(conv6)
    conv6 = keras.layers.Dropout(0.1)(conv6)

    # Output
    outputs = keras.layers.Conv2D(3, (1, 1), activation='sigmoid')(conv6)

    return keras.models.Model(inputs=[base_model.input], outputs=[outputs])

keras.backend.clear_session()
model = unet_vgg()
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])

# fit the model on the batches generated by datagen.flow()
history = model.fit(train_masked, validation_data=test_masked, 
          epochs=200, 
          steps_per_epoch=len(train_masked), 
          validation_steps=len(test_masked),
          use_multiprocessing=True)

# Save it under the form of a json file
history_dict = history.history
json.dump(history_dict, open('history.json', 'w'))

plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.savefig('unet_vgg_model_loss.png')

plt.figure(figsize=(12, 6))
plt.plot(history.history['dice_coef'])
plt.plot(history.history['val_dice_coef'])
plt.title('Model Dice Coefficient')
plt.ylabel('Dice Coefficient')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.savefig('unet_vgg_model_dice_coef.png')

model.save('unet_vgg_model.h5')


